{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBEEEaaHYOp6",
        "outputId": "0bb72b71-c985-4acd-de6b-1123be2018e1"
      },
      "outputs": [],
      "source": [
        "# Colab 运行前必跑，安装需要的库\n",
        "!pip install -q markdownify # requests tqdm\n",
        "!apt -yqq install fonts-noto-cjk # 安装字体"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM6ABVi2TN88"
      },
      "source": [
        "如果你想要的Tag有很多博文，建议按 月 1/2 博文类型 逐个解析，这样能获取最多的内容，有能力也可以自己写个循环\n",
        "\n",
        " ```python\n",
        " # 例如：\n",
        " 标签 = \"真三国无双\";  筛选年月 = 202503;  博文类型 = \"1\"\n",
        " 标签 = \"真三国无双\";  筛选年月 = 202503;  博文类型 = \"2\"\n",
        " ```\n",
        "\n",
        " 会检查已下载的图片，如果大小相同会直接跳过\n",
        "\n",
        " 可以设置的参数：\n",
        "\n",
        "| 变量名 | 类型 | 介绍 | 示例值 |\n",
        "|--------|------|------|--------|\n",
        "| 标签 | string | 搜索的主标签 | `\"真三国无双\"` |\n",
        "| 博文类型 | string | `空字符`：全部 `1`：文字 `2`：图片 `4`: 视频 | `0` |\n",
        "| 筛选年月 | integer | `0`表示不筛选，或使用`YYYYMM`格式 | `202503` |\n",
        "| **筛选参数** |||\n",
        "| 白名单 | string | 只要包含其中一个标签就行（逗号或空格分隔） | `\"绘图，3D\"` |\n",
        "| 完全白名单 | string | 必须包含所有标签才行（逗号或空格分隔） | `\"曹操，郭嘉\"` |\n",
        "| 黑名单 | string | 包含其中一个标签就不行（逗号或空格分隔） | `\"剧透，攻略\"` |\n",
        "| 最低热度 | integer | 低于这个热度的博文将被剔除 | `500` |\n",
        "| 文本内容字数限制 | integer | 超过这个字数的博文将被剔除（仅在写入时生效） | `50` |\n",
        "| **下载参数** |||\n",
        "| 连接的最大线程数 | integer | 最大线程数（建议4-8） | `4` |\n",
        "| 是否写文的内容 | boolean | 是否写入文本内容 | `False` |\n",
        "| 是否写图的内容 | boolean | 是否写入图片内容 | `False` |\n",
        "| 是否下载图片 | boolean | 是否下载图片 | `False` |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcZjwfDmNRU3",
        "outputId": "9f589a97-66be-4b6f-d61e-85c496e3d782"
      },
      "outputs": [],
      "source": [
        "#!/bin/env python3\n",
        "#################################################################################\n",
        "# 这个脚本是一个 Lofter 博客下载器，主要用于下载和解析 Lofter 上的热门标签博文。\n",
        "# 该脚本支持多线程下载图片，并提供了自定义文件名和筛选器的功能。\n",
        "#\n",
        "# 1. 按照 年月-博文类型 解析标签下该月的博文 (最多1000条，服务器端的限制，没有办法绕过)\n",
        "# 2. 解析完后，获取感兴趣的博文的详情\n",
        "# 3. 按照要求，按需下载博文的图片和文本内容\n",
        "#\n",
        "# 建议使用 Python 3.10 以上版本 （使用了类型注解）\n",
        "# 需要安装的包 pip install requests  tqdm   markdownify\n",
        "#                        网络请求  进度条   Markdown 转换\n",
        "#################################################################################\n",
        "import os, time, re\n",
        "\n",
        "输出文件夹 = \".\"        # . 为当前目录\n",
        "标签 = \"真三国无双\" # @param {type:\"string\"}\n",
        "\n",
        "# **筛选的年月** `0`：不筛选 或 `YYYYMM` 格式，例如 `202503` 2025年3月\n",
        "筛选的年月 = 202504 # @param {\"type\":\"integer\"}\n",
        "\n",
        "# **博文类型** `空字符`：全部 `1`：文字 `2`：图片\n",
        "博文类型 = \"\" # @param {\"type\":\"string\"}\n",
        "\n",
        "# 下面的设置会影响输出 但不会影响JSON解析 （只有把所有内容下完之后，才能进行筛选）\n",
        "白名单 = \"\"      # @param {\"type\":\"string\"}\n",
        "完全白名单 = \"\"      # @param {\"type\":\"string\"}\n",
        "黑名单 = \"\"      # @param {\"type\":\"string\"}\n",
        "最低热度  = 5000      # @param {\"type\":\"integer\"}\n",
        "文本内容字数限制 = 0      # @param {\"type\":\"integer\"}\n",
        "\n",
        "# 下载 建议 4-8 线程\n",
        "连接的最大线程数 = 4      # @param {\"type\":\"integer\"}\n",
        "是否写文的内容 = False      # @param {\"type\":\"boolean\"}\n",
        "是否写图的内容 = False      # @param {\"type\":\"boolean\"}\n",
        "是否下载图片 = False      # @param {\"type\":\"boolean\"}\n",
        "\n",
        "# 解析进度保存的 JSON 文件\n",
        "backup_list_json_path =  os.path.join(\n",
        "    输出文件夹,\n",
        "    f\"{标签}_{博文类型}_{筛选的年月}.json.gz\" # 保存解析进度到这个json文件\n",
        ")\n",
        "\"\"\" 这个 JSON 文件是一个字典 存有两个值 data (已解析的内容) 和 offset (当前解析进度)\n",
        "\n",
        "data 也是一个字典，所包含的键为：\n",
        "    'post_id':            博文ID\n",
        "    'type':               博文类型  1: 文本 2: 图片 4: 视频 （目前只处理 1 和 2）\n",
        "    'blog_id':            用户ID\n",
        "    'blog_user_name':     用户名 用于访问ta的主页\n",
        "    'blog_user_nickname': 用户的昵称 （显示的名字）\n",
        "    'publish_datetime':   博文发出时间  YYYY-MM-DD HH:MM:SS\n",
        "    'publish_timestamp':  博文发出时间戳 ms 毫秒\n",
        "    'tag_list':           所有标签 (List)\n",
        "    'response_count':     回复数\n",
        "    'favorite_count':     喜欢数\n",
        "    'share_count':        分享数\n",
        "    'hot_count':          热度\n",
        "    'permalink':          暂时存着，将来应该有用\n",
        "    'title':              博文标题\n",
        "    'has_detail':         是否有详情 用于判断是否解析过详情\n",
        "    'content':            博文内容 (解析JSON时为预览内容，并不是完整内容)\n",
        "    'photo_links':        图片链接 (解析JSON时为空列表) []\n",
        "\n",
        "如果 offset 为 -1 代表解析完成\n",
        "\"\"\"\n",
        "\n",
        "# region 自定义\n",
        "output_folder = 输出文件夹\n",
        "tag_name = 标签.strip() # 标签名称，去除首尾空格\n",
        "post_year_month = 筛选的年月\n",
        "post_types = 博文类型\n",
        "\n",
        "# 下面的设置会影响输出 但不会影响JSON解析 （只有把所有内容下完之后，才能进行筛选）\n",
        "white_list = 白名单\n",
        "white_list = set(item.strip() for item in re.split(r'[,，;； ]+', white_list) if item)\n",
        "complete_white_list = 完全白名单\n",
        "complete_white_list = set(item.strip() for item in re.split(r'[,，;； ]+', complete_white_list) if item)\n",
        "black_list = 黑名单\n",
        "black_list = set(item.strip() for item in re.split(r'[,，;； ]+', black_list) if item)\n",
        "min_hot  = 最低热度\n",
        "text_word_limit = 文本内容字数限制\n",
        "\n",
        "# 下载设置\n",
        "max_workers = 连接的最大线程数\n",
        "is_write_txt = 是否写文的内容\n",
        "is_write_image_txt = 是否写图的内容\n",
        "is_download_image = 是否下载图片\n",
        "\n",
        "# 其他设置/参数\n",
        "skip_parsed_time = 0.01   # 跳过已解析的时间，单位秒，避免输出过快\n",
        "\n",
        "# 统计信息相关\n",
        "n_post_type_skipped = 0   # 博文类型跳过的数量\n",
        "n_min_host_skipped = 0    # 最小热度跳过的数量\n",
        "n_complete_white_list_skipped = 0  # 完全白名单跳过的数量\n",
        "n_white_list_skipped = 0  # 白名单跳过的数量\n",
        "n_black_list_skipped = 0  # 黑名单跳过的数量\n",
        "\n",
        "import os, time, json, gzip, re, requests\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from tqdm import tqdm\n",
        "import urllib.parse\n",
        "from markdownify import markdownify\n",
        "from copy import deepcopy\n",
        "\n",
        "def custom_file_name(\n",
        "        post_id, type, blog_id, blog_user_name, blog_user_nickname,\n",
        "        publish_datetime, publish_timestamp, tag_list, response_count,\n",
        "        favorite_count, share_count, hot_count, permalink, title,\n",
        "        has_detail, content, photo_links\n",
        "    ) -> str:\n",
        "    \"\"\"\n",
        "    自定义输出文件名   发挥主观能动性，并用AI辅助吧！\n",
        "\n",
        "    可使用的参数可以看上面的注释，注意要使用 safe_name()\n",
        "    \"\"\"\n",
        "\n",
        "    safe_blog_user_nickname = safe_name(blog_user_nickname)\n",
        "    post_datetime = datetime.fromtimestamp(publish_timestamp / 1000)\n",
        "    post_time = post_datetime.strftime('%Y%m%d_%H%M')\n",
        "    year_month = post_datetime.strftime(\"%Y%m\")\n",
        "    safe_title = safe_name(title) if title else \"无标题\"\n",
        "\n",
        "    # 标签/类型/YYYYMM/昵称_时间_类型_热度_标题\n",
        "    return os.path.join(\n",
        "        tag_name,\n",
        "        get_post_types_name(type),\n",
        "        year_month,\n",
        "        f\"{safe_blog_user_nickname}_{post_time}_{get_post_types_name(type)}_{hot_count}_{safe_title}\"\n",
        "    )\n",
        "\n",
        "    # 标签/昵称/昵称_时间_类型_热度_标题\n",
        "    return os.path.join(\n",
        "        tag_name,\n",
        "        safe_blog_user_nickname,\n",
        "        f\"{safe_blog_user_nickname}_{post_time}_{get_post_types_name(type)}_{hot_count}_{safe_title}\"\n",
        "    )\n",
        "\n",
        "    # 这是我的例子： 标签/YYYYMM/昵称_时间_类型_热度_标题\n",
        "    return os.path.join(\n",
        "        tag_name,\n",
        "        year_month,\n",
        "        f\"{safe_blog_user_nickname}_{post_time}_{get_post_types_name(type)}_{hot_count}_{safe_title}\"\n",
        "    )\n",
        "\n",
        "def custom_stats(backup_list: list[dict]) -> None:\n",
        "    \"\"\"\n",
        "    自定义统计函数，打印并保存一些统计信息\n",
        "\n",
        "    Args:\n",
        "        backup_list (List[dict]): 备份列表，包含所有博文的字典列表。\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=============================================\")\n",
        "    print(f\"[统计] {tag_name} {post_year_month}: 热度 {min_hot} 白名单 {white_list} 黑名单 {black_list} 筛选后\")\n",
        "    print(f\"筛选顺序: 博文类型 -> 热度 -> 白名单 -> 完全白名单 -> 黑名单\")\n",
        "    print(f\"博文类型跳过: {n_post_type_skipped} 热度跳过: {n_min_host_skipped} 白名单跳过: {n_white_list_skipped} 完全白名单跳过: {n_complete_white_list_skipped} 黑名单跳过: {n_black_list_skipped}\")\n",
        "\n",
        "    n_post_img = n_post_txt = n_post_video = 0\n",
        "\n",
        "    for backup_dict in backup_list:\n",
        "        post_type = get_post_types_name(backup_dict['type'])  # \"文字\" 或 \"图片\"\n",
        "        if post_type == \"图片\":\n",
        "            n_post_img += 1\n",
        "        elif post_type == \"文字\":\n",
        "            n_post_txt += 1\n",
        "        elif post_type == \"视频\":\n",
        "            n_post_video += 1\n",
        "\n",
        "    print(f\"博文数量: {len(backup_list)} 图:{n_post_img} 文:{n_post_txt} 视频:{n_post_video}\")\n",
        "\n",
        "def custom_filter(\n",
        "        post_id, type, blog_id, blog_user_name, blog_user_nickname,\n",
        "        publish_datetime, publish_timestamp, tag_list, response_count,\n",
        "        favorite_count, share_count, hot_count, permalink, title,\n",
        "        has_detail, content, photo_links\n",
        "    ) -> bool:\n",
        "    \"\"\"\n",
        "    自定义的筛选器   发挥主观能动性，并用AI辅助吧！ （只会在生成文本和下载图片时使用，解析JSON时不会生效）\n",
        "\n",
        "    返回 True 保留该条数据，返回 False 剔除该条数据\n",
        "    \"\"\"\n",
        "    global n_post_type_skipped, n_min_host_skipped, n_white_list_skipped, n_black_list_skipped, n_complete_white_list_skipped\n",
        "\n",
        "    # 我来举个例子吧：仅在需要时写入\n",
        "    if is_write_txt or is_write_image_txt or is_download_image:\n",
        "        if type == 1 and not is_write_txt:\n",
        "            n_post_type_skipped += 1\n",
        "            return False\n",
        "        elif type == 2 and not is_write_image_txt and not is_download_image:\n",
        "            n_post_type_skipped += 1\n",
        "            return False\n",
        "        elif type == 4: # 视频类型 直接跳过\n",
        "            n_post_type_skipped += 1\n",
        "            return False\n",
        "\n",
        "    # 热度大于 你设定的值\n",
        "    if hot_count < min_hot:\n",
        "        n_min_host_skipped += 1\n",
        "        return False  # 跳过该条数据\n",
        "\n",
        "    # 有想要的标签 在白名单里\n",
        "    if white_list:\n",
        "        if (len(set(tag_list).intersection(white_list)) == 0):\n",
        "            n_white_list_skipped += 1\n",
        "            return False  # 跳过该条数据\n",
        "\n",
        "    # 完全白名单\n",
        "    if complete_white_list:\n",
        "        if not set(tag_list).issuperset(complete_white_list):\n",
        "            n_complete_white_list_skipped += 1\n",
        "            return False  # 跳过该条数据\n",
        "\n",
        "    # 没有不想要的标签 在黑名单里\n",
        "    if black_list:\n",
        "        if (len(set(tag_list).intersection(black_list)) > 0):\n",
        "            n_black_list_skipped += 1\n",
        "            return False  # 跳过该条数据\n",
        "\n",
        "    # 你可以在这里添加更多的筛选条件，比如转发数、评论数等\n",
        "\n",
        "    return True # 保留该条数据\n",
        "\n",
        "def custom_filter_for_text(content: str) -> bool:\n",
        "    \"\"\"\n",
        "    自定义的文本筛选器\n",
        "\n",
        "    Args:\n",
        "        content (str): 博文的文本内容。\n",
        "\n",
        "    Returns:\n",
        "        bool: 是否保留该条数据\n",
        "    \"\"\"\n",
        "    # 移除 YAML front matter (元数据)\n",
        "    content = re.sub(r'^---\\n.*?\\n---\\n', '', content, flags=re.DOTALL)\n",
        "\n",
        "    # 移除 Markdown 修饰符\n",
        "    content = re.sub(r'[#*_\\-`~]', '', content)  # 标题、加粗、斜体、列表等标记\n",
        "    content = re.sub(r'!?\\[.*?\\]\\(.*?\\)', '', content)  # 图片和链接\n",
        "    content = re.sub(r'<!--.*?-->', '', content, flags=re.DOTALL)  # HTML注释\n",
        "\n",
        "    # 移除多余的空格和空行\n",
        "    content = re.sub(r'\\s+', ' ', content).strip()\n",
        "\n",
        "    # 统计字符数\n",
        "    return len(content) >= text_word_limit\n",
        "\n",
        "def custom_content_process(\n",
        "        post_id, type, blog_id, blog_user_name, blog_user_nickname,\n",
        "        publish_datetime, publish_timestamp, tag_list, response_count,\n",
        "        favorite_count, share_count, hot_count, permalink, title,\n",
        "        has_detail, content, photo_links\n",
        "    ) -> str:\n",
        "    \"\"\"\n",
        "    自定义 HTML 内容处理，你可以使用 BS4 之类的库做进一步处理\n",
        "\n",
        "    我这里是 将博客 HTML 内容转换为 Markdown 格式，并移除多余的格式符号。\n",
        "\n",
        "    Args:\n",
        "        content (str): 博客的 HTML 格式正文内容。\n",
        "\n",
        "    Returns:\n",
        "        str: 处理后的内容\n",
        "    \"\"\"\n",
        "\n",
        "    content = markdownify(content).replace(\"****\", \"\") # 转成makrdown而不是纯文本，用于保留链接、格式等信息，避免结构丢失\n",
        "    tag_lines = \"\\n  - \" + \"\\n  - \".join(tag_list) if tag_list else \"null\"\n",
        "    photo_lines = \"\\n  - \" + \"\\n  - \".join(photo_links) if photo_links else \"null\"\n",
        "\n",
        "    return f\"\"\"---\n",
        "博文id: {post_id}\n",
        "博文类型：{get_post_types_name(type)}\n",
        "用户id: {blog_id}\n",
        "用户名: {blog_user_name}\n",
        "用户昵称: {blog_user_nickname}\n",
        "发布时间: \"{publish_datetime}\"\n",
        "标签:{tag_lines}\n",
        "热度: {hot_count}\n",
        "图片链接:{photo_lines}\n",
        "---\n",
        "\n",
        "# {title}\n",
        "\n",
        "{content}\n",
        "\"\"\"\n",
        "# endregion\n",
        "\n",
        "# region 关键代码\n",
        "HOT_TAG_API_URL = \"https://api.lofter.com/newapi/tagPosts.json\"\n",
        "POST_DETIAL_API_URL = \"https://api.lofter.com/oldapi/post/detail.api\"\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        backup_list = parse_posts_json()\n",
        "\n",
        "        if not backup_list:\n",
        "            print(\"[解析异常] 未检测到任何博客，请检查设置的 名称 或 时间 或 博文类型\")\n",
        "            return\n",
        "\n",
        "        if is_write_txt or is_write_image_txt or is_download_image:\n",
        "\n",
        "            print(f\"[获取详细博文] 白名单: {white_list}, 黑名单: {black_list}, 热度: {min_hot}\")\n",
        "            # backup_list = fetch_missing_details(backup_list, max_workers=max_workers) # 获取全部缺失博文详情\n",
        "            backup_list = fetch_details_if_pass_filter(backup_list, max_workers=max_workers) # 获取筛选后的博文详情\n",
        "            global parsed_year_month\n",
        "            save_list_offset_to_json(backup_list, -1, backup_list_json_path) # 保存解析进度到JSON文件\n",
        "\n",
        "            backup_list = list(filter(lambda x: custom_filter(**x), backup_list)) # 过滤掉不需要的博文\n",
        "\n",
        "            global n_post_type_skipped, n_min_host_skipped, n_white_list_skipped, n_black_list_skipped, n_complete_white_list_skipped\n",
        "            n_post_type_skipped //= 2  # 筛了两遍所以除以2\n",
        "            n_min_host_skipped //= 2\n",
        "            n_white_list_skipped //= 2\n",
        "            n_black_list_skipped //= 2\n",
        "            n_complete_white_list_skipped //= 2\n",
        "\n",
        "            image_download_list = []  # List[Tuple[str, str]]\n",
        "            print(f\"[写入文本中] {tag_name} ...\")\n",
        "\n",
        "            for backup_dict in backup_list:\n",
        "\n",
        "                global output_folder\n",
        "                file_name = os.path.join(output_folder, custom_file_name(**backup_dict))\n",
        "\n",
        "                # 写入文本内容\n",
        "                content_path = file_name + \".txt\"\n",
        "                if backup_dict['type'] == 1 and is_write_txt:\n",
        "                    write_content(backup_dict, content_path)\n",
        "                elif backup_dict['type'] == 2 and is_write_image_txt:\n",
        "                    write_content(backup_dict, content_path)\n",
        "\n",
        "                if is_download_image: # 下载图片\n",
        "                    for idx, url in enumerate(backup_dict['photo_links']):\n",
        "                        filename = f\"{file_name}_{idx + 1}\" + get_file_extension_from_url(url)\n",
        "                        image_download_list.append((url, filename))\n",
        "\n",
        "            # 下载图片并显示进度条\n",
        "            print(f\"[下载图片] {tag_name}: 图片数量: {len(image_download_list)}\")\n",
        "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "                futures = {\n",
        "                    executor.submit(download_image, url, output_path): (url, output_path)\n",
        "                    for url, output_path in image_download_list\n",
        "                }\n",
        "\n",
        "                for future in tqdm(as_completed(futures), total=len(futures), desc=\"下载图片\"):\n",
        "                    url, output_path = futures[future]\n",
        "                    try:\n",
        "                        success = future.result()\n",
        "                        if not success:\n",
        "                            print(f\"下载失败: {url}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"下载异常: {url} -> {e}\")\n",
        "        else:\n",
        "            # 没有要写入的，但还是要筛选\n",
        "            backup_list = list(filter(lambda x: custom_filter(**x), backup_list))\n",
        "\n",
        "        custom_stats(backup_list)  # 自定义统计信息\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n用户中断（Ctrl+C），正在安全退出...\")\n",
        "\n",
        "def parse_posts_json():\n",
        "    \"\"\"\n",
        "    像服务器请求相关内容\n",
        "\n",
        "    Returns:\n",
        "        List 包含了所有博文的一个列表\n",
        "    \"\"\"\n",
        "\n",
        "    n_post_img = n_post_txt = 0\n",
        "    global tag_name, post_types, post_year_month, backup_list_json_path\n",
        "\n",
        "    # 继续之前中断的下载\n",
        "    backup_list, offset = load_list_offset_from_json(backup_list_json_path)\n",
        "    username_blogid_postid_set = set(get_username_blogid_postid_from_backup_dict(backup_dict) for backup_dict in backup_list)\n",
        "    for backup_dict in backup_list:\n",
        "        if backup_dict[\"type\"] == 1:\n",
        "            n_post_txt += 1\n",
        "        elif backup_dict[\"type\"] == 2:\n",
        "            n_post_img += 1\n",
        "\n",
        "    if offset == -1 and len(backup_list) != 0:\n",
        "        print(f\"[跳过JSON下载与解析] 标签: {tag_name}, 图片:{n_post_img}, 文字:{n_post_txt}\")\n",
        "        return backup_list\n",
        "\n",
        "    # region 解析 JSON\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        print(\"开始下载与解析JSON，按 Ctrl+C 终止...\")\n",
        "        print(f\"标签: {tag_name}, 博客类型: {get_post_types_name(post_types)}, 搜索年月: {post_year_month}\")\n",
        "        session = requests.Session()\n",
        "        while True:\n",
        "            print(f\"\\r[解析中] {tag_name}: {post_year_month} {get_post_types_name(post_types)}, 偏移：{offset} 图:{n_post_img}, 文:{n_post_txt}, 运行时间: {format_elapsed_time(time.time() - start_time)}\", end=\"\", flush=True)\n",
        "\n",
        "            response = session.post(\n",
        "                url = HOT_TAG_API_URL,\n",
        "                headers = get_headers(),\n",
        "                data = get_hot_tag_payload(\n",
        "                    tag = tag_name,\n",
        "                    offset = offset,\n",
        "                    post_types = post_types,\n",
        "                    post_year_month = post_year_month\n",
        "                )\n",
        "            )\n",
        "\n",
        "            response_json = response.json()\n",
        "\n",
        "            if response_json[\"msg\"] != \"成功\":\n",
        "                raise ValueError(\"请求结果不成功\")\n",
        "\n",
        "            for info_dict in response_json['data']['list']:\n",
        "                backup_dict = get_backup_dict_from_info_dict(info_dict)\n",
        "                if not backup_dict:\n",
        "                    raise ValueError\n",
        "\n",
        "                username_blogid_postid = get_username_blogid_postid_from_backup_dict(backup_dict)\n",
        "\n",
        "                if username_blogid_postid not in username_blogid_postid_set:\n",
        "                    backup_list.append(backup_dict)\n",
        "\n",
        "                    if backup_dict[\"type\"] == 1:\n",
        "                        n_post_txt += 1\n",
        "                    elif backup_dict[\"type\"] == 2:\n",
        "                        n_post_img += 1\n",
        "\n",
        "            if response_json['data']['offset'] == -1:\n",
        "                offset = -1\n",
        "                break\n",
        "\n",
        "            # 保存到列表后 更新 offset\n",
        "            offset = response_json['data']['offset']\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(f\"\\n[解析中断] 检测到 Ctrl+C，正在保存数据...\\n标签: {tag_name}, 博客类型: {post_types}, 搜索年月: {post_year_month}, offset: {offset}\")\n",
        "        raise # 继续向上抛出异常\n",
        "\n",
        "    except (ConnectionError, TimeoutError, requests.ReadTimeout) as e:\n",
        "        if isinstance(e, ConnectionError):\n",
        "            errorType = \"连接失败\"\n",
        "        elif isinstance(e, TimeoutError):\n",
        "            errorType = \"连接超时\"\n",
        "        else:\n",
        "            errorType = \"读取超时\"\n",
        "\n",
        "        print(f\"\\n[{errorType}] 标签: {tag_name}, 博客类型: {post_types}, 搜索年月: {post_year_month}, offset: {offset}\")\n",
        "\n",
        "    except ValueError:\n",
        "        print(f\"\\n[请求错误] 标签: {tag_name}, 博客类型: {post_types}, 搜索年月: {post_year_month}, offset: {offset}\")\n",
        "\n",
        "    finally:\n",
        "        if backup_list:\n",
        "            save_list_offset_to_json(backup_list, offset, backup_list_json_path)\n",
        "            print(f\"\\n[解析完成] 数据保存完毕 {backup_list_json_path}\")\n",
        "    # endregion\n",
        "\n",
        "    return(backup_list)\n",
        "\n",
        "# region 方法-请求\n",
        "def format_elapsed_time(seconds: float) -> str:\n",
        "    \"\"\"\n",
        "    将运行时间（秒）格式化为 \"HH:MM:SS\" 字符串。\n",
        "\n",
        "    Args:\n",
        "        seconds (float): 运行时间的总秒数。\n",
        "\n",
        "    Returns:\n",
        "        str: 格式化后的字符串，格式为 \"小时:分钟:秒\"，例如 \"01:23:45\"。\n",
        "    \"\"\"\n",
        "    total_seconds = int(seconds)\n",
        "    h = total_seconds // 3600\n",
        "    m = (total_seconds % 3600) // 60\n",
        "    s = total_seconds % 60\n",
        "    return f\"{h:02}:{m:02}:{s:02}\"\n",
        "\n",
        "def get_post_types_name(post_types: str) -> str:\n",
        "    \"\"\"\n",
        "    根据博文类型代码返回对应名称。\n",
        "\n",
        "    Args:\n",
        "        post_types (str): 博文类型代码，\"1\" 表示文字，\"2\" 表示图片，\"4\" 表示视频，其他或空为所有。\n",
        "\n",
        "    Returns:\n",
        "        str: 博文类型名称。\n",
        "    \"\"\"\n",
        "    type_map = {\n",
        "        \"1\": \"文字\",\n",
        "        \"2\": \"图片\",\n",
        "        \"4\": \"视频\",\n",
        "    }\n",
        "\n",
        "    return type_map.get(str(post_types), \"所有\")\n",
        "\n",
        "def fix_image_url(url: str) -> str:\n",
        "    \"\"\"\n",
        "    如果url包含 nos.netease.com，提取其中 imglf\\\\d+ 部分，\n",
        "    并对URL做自定义替换或拼接。\n",
        "\n",
        "    例子：\n",
        "        输入:\n",
        "            http://nos.netease.com/imglf5/img/4fc1c0791abd2d38/...\n",
        "        输出:\n",
        "            https://imglf5.lf127.net/img/4fc1c0791abd2d38/...\n",
        "    \"\"\"\n",
        "\n",
        "    if \"nos.netease.com\" in url:\n",
        "        start_idx = url.find(\"imglf\")\n",
        "        if start_idx == -1:\n",
        "            return url\n",
        "\n",
        "        end_idx = url.find(\"/\", start_idx)\n",
        "        if end_idx == -1:\n",
        "            return url\n",
        "\n",
        "        imglf_part = url[start_idx:end_idx]\n",
        "\n",
        "        img_path_idx = url.find(\"img/\", end_idx)\n",
        "        if img_path_idx == -1:\n",
        "            return url\n",
        "\n",
        "        path_part = url[img_path_idx + len(\"img/\"):]\n",
        "        fixed_url = f\"https://{imglf_part}.lf127.net/img/{path_part}\"\n",
        "        return fixed_url\n",
        "\n",
        "    return url\n",
        "\n",
        "def get_username_blogid_postid_from_backup_dict(backup_dict: dict) -> str:\n",
        "    \"\"\"\n",
        "    提取用户名、博客ID和博文ID，格式为 username_blogid_postid。\n",
        "\n",
        "    Args:\n",
        "        backup_dict (dict): 包含博文信息的备份字典。\n",
        "\n",
        "    Returns:\n",
        "        str: 组合后的字符串，如 username_123456_987654321。\n",
        "    \"\"\"\n",
        "\n",
        "    return f\"{backup_dict['blog_user_name']}_{backup_dict['blog_id']}_{backup_dict['post_id']}\"\n",
        "\n",
        "def get_file_extension_from_url(url: str) -> str:\n",
        "    \"\"\"\n",
        "    从一个 URL 提取文件扩展名（如 .jpg、.png）\n",
        "\n",
        "    参数:\n",
        "        url (str): 包含文件路径的 URL 字符串\n",
        "\n",
        "    返回:\n",
        "        str: 文件扩展名（包含点号），如果没有则返回空字符串\n",
        "    \"\"\"\n",
        "    path = urllib.parse.urlparse(url).path\n",
        "    return Path(path).suffix\n",
        "\n",
        "def get_headers() -> dict:\n",
        "    \"\"\"\n",
        "    获取请求所需的 HTTP 头部。\n",
        "\n",
        "    Returns:\n",
        "        dict: 包含请求头字段的字典。\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"Accept\": '*/*',\n",
        "        \"Accept-Encoding\": \"gzip\",\n",
        "        \"Accept-Language\": \"zh-CN,zh;q=0.9\",\n",
        "        \"Connection\": \"keep-alive\",\n",
        "        \"Content-Type\": \"application/x-www-form-urlencoded; charset=utf-8\",\n",
        "        \"User-Agent\": \"User-Agent: LOFTER-Android 8.1.28 (V2309A; Android 9; null) WIFI\"\n",
        "    }\n",
        "\n",
        "def get_hot_tag_payload(tag: str, offset:int = 0, post_types = \"\", post_year_month: int = 0) -> dict:\n",
        "    \"\"\"\n",
        "    构建请求标签列表接口的 POST 数据负载。\n",
        "\n",
        "    Args:\n",
        "        tag (str): 标签名称。\n",
        "        offset (int, optional): 请求偏移量，默认从0开始，-1表示无内容。\n",
        "        post_types (str, optional): 文章类型过滤，空字符串代表全部，\"1\"代表文字，\"2\"代表图片。\n",
        "        post_year_month (int, optional): 搜索的年月，格式为 YYYYMM，如 202504，默认为0表示不过滤。\n",
        "\n",
        "    Returns:\n",
        "        dict: 用于 POST 请求的参数字典。\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"product\": \"lofter-android-8.1.28\",\n",
        "        \"postTypes\": post_types, # 空：全部 1：文字 2：图片\n",
        "        \"offset\": offset,\n",
        "        \"postYm\": post_year_month if post_year_month != 0 else \"\",\n",
        "        \"returnGiftCombination\": \"\",\n",
        "        \"recentDay\": 0,\n",
        "        \"protectedFlag\": 0,\n",
        "        \"range\": 0,\n",
        "        \"firstpermalink\": \"null\",\n",
        "        \"style\": 0,\n",
        "        \"tag\": tag,\n",
        "        \"type\": \"total\", # \"total\"：全部 \"date\": 日榜 \"week\": 周榜 \"month\": 月榜\n",
        "    }\n",
        "\n",
        "def get_post_detail_payload(tag, post_id, blog_id, blog_user_name: str) -> dict:\n",
        "    \"\"\"\n",
        "    构建请求单篇博文详情接口的 POST 数据负载。\n",
        "\n",
        "    Args:\n",
        "        tag (str): 标签名称。\n",
        "        post_id (int): 博文ID。\n",
        "        blog_id (int): 博主ID。\n",
        "        blog_user_name (str): 博主用户名，用于构造博客域名。\n",
        "\n",
        "    Returns:\n",
        "        dict: 用于 POST 请求的参数字典。\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"targetblogid\": blog_id, # ['postData']['postView']['blogId']\n",
        "        \"supportposttypes\": \"1,2,3,4,5,6\",\n",
        "        \"tagFrom\": tag,\n",
        "        \"blogdomain\": f\"{blog_user_name}.lofter.com\", # ['blogInfo']['blogName']\n",
        "        \"offset\": \"0\",\n",
        "        \"requestType\": \"0\",\n",
        "        \"postdigestnew\": \"1\",\n",
        "        \"postid\": post_id, # ['postData']['postView']['id']\n",
        "        \"checkpwd\": \"1\",\n",
        "        \"needgetpoststat\": \"1\"\n",
        "  }\n",
        "\n",
        "def get_post_detail_from_info_dict(info_dict) -> dict | None:\n",
        "    \"\"\"\n",
        "    根据热标签接口返回的博文信息字典，调用详情接口获取完整博文详情。\n",
        "\n",
        "    Args:\n",
        "        info_dict (dict): 由热标签接口返回的数据中某一项，包含博文基础信息。\n",
        "\n",
        "    Returns:\n",
        "        dict: 返回包含博文详情的字典，格式如下：\n",
        "            - type (int): 博文类型，1为文字，2为图片。\n",
        "            - content (str): 博文内容。\n",
        "            - photo_links (list): 图片链接列表（如果有）。\n",
        "        如果请求失败或数据异常，返回 None。\n",
        "    \"\"\"\n",
        "    detail_response = requests.post(\n",
        "        url = POST_DETIAL_API_URL,\n",
        "        headers = get_headers(),\n",
        "        data = get_post_detail_payload(\n",
        "            tag = tag_name,\n",
        "            post_id = info_dict['postData']['postView']['id'],\n",
        "            blog_id = info_dict['postData']['postView']['blogId'],\n",
        "            blog_user_name = info_dict['blogInfo']['blogName']\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if detail_response.status_code != 200:\n",
        "        return None\n",
        "\n",
        "    detail_response_json = detail_response.json()\n",
        "\n",
        "    # 如果结果里面没有 posts\n",
        "    posts = detail_response_json.get('response', {}).get('posts', [])\n",
        "    if not posts:\n",
        "        return None\n",
        "\n",
        "    detail_dict = detail_response_json['response']['posts'][0]['post']\n",
        "\n",
        "    photo_links = []\n",
        "\n",
        "    # 图片博客\n",
        "    if detail_dict['type'] == 2:\n",
        "        try:\n",
        "            photo_links = [photo['raw'] for photo in json.loads(detail_dict['photoLinks'])]\n",
        "            photo_links = [fix_image_url(url) for url in photo_links] # fix the old image url\n",
        "        except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
        "            photo_links = []\n",
        "\n",
        "    return {\n",
        "        \"type\": detail_dict['type'],\n",
        "        \"content\": detail_dict['content'],\n",
        "        \"photo_links\": photo_links,\n",
        "    }\n",
        "\n",
        "def get_post_detail_from_backup_dict(backup_dict: dict) -> dict | None:\n",
        "    \"\"\"\n",
        "    根据备份字典获取博文详情。\n",
        "\n",
        "    Args:\n",
        "        backup_dict (dict): 备份字典，包含博文的基本信息。\n",
        "\n",
        "    Returns:\n",
        "        dict: 返回包含博文详情的字典，格式如下：\n",
        "            - type (int): 博文类型，1为文字，2为图片。\n",
        "            - content (str): 博文内容。\n",
        "            - photo_links (list): 图片链接列表（如果有）。\n",
        "        如果请求失败或数据异常，返回 None。\n",
        "    \"\"\"\n",
        "    detail_response = requests.post(\n",
        "        url = POST_DETIAL_API_URL,\n",
        "        headers = get_headers(),\n",
        "        data = get_post_detail_payload(\n",
        "            tag = tag_name,\n",
        "            post_id = backup_dict['post_id'],\n",
        "            blog_id = backup_dict['blog_id'],\n",
        "            blog_user_name = backup_dict['blog_user_name']\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if detail_response.status_code != 200:\n",
        "        return None\n",
        "\n",
        "    detail_response_json = detail_response.json()\n",
        "\n",
        "    # 如果结果里面没有 posts\n",
        "    posts = detail_response_json.get('response', {}).get('posts', [])\n",
        "    if not posts:\n",
        "        return None\n",
        "\n",
        "    detail_dict = detail_response_json['response']['posts'][0]['post']\n",
        "\n",
        "    photo_links = []\n",
        "\n",
        "    # 图片博客\n",
        "    if detail_dict['type'] == 2:\n",
        "        try:\n",
        "            photo_links = [photo['raw'] for photo in json.loads(detail_dict['photoLinks'])]\n",
        "            photo_links = [fix_image_url(url) for url in photo_links] # fix the old image url\n",
        "        except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
        "            photo_links = []\n",
        "\n",
        "    return {\n",
        "        \"type\": detail_dict['type'],\n",
        "        \"content\": detail_dict['content'],\n",
        "        \"photo_links\": photo_links,\n",
        "    }\n",
        "\n",
        "def get_backup_dict_from_info_dict(info_dict: dict) -> dict:\n",
        "    \"\"\"\n",
        "    根据从 HOT_TAG_API_URL 获取的 info_dict 构造备份字典，包含博文的基本信息与详情内容。\n",
        "    若第一次请求详情失败，将重试一次，若仍失败则返回 None。\n",
        "\n",
        "    Args:\n",
        "        info_dict (dict): 单条博文信息，来自 HOT_TAG_API_URL 的响应结构，如 response.json()['data']['list'][i]。\n",
        "\n",
        "    Returns:\n",
        "        dict or None: 成功时返回备份信息字典，失败返回 None。备份信息包括用户、标题、内容、标签、图片链接等。\n",
        "    \"\"\"\n",
        "    # 获取详情（最多尝试两次） 会显著增加请求时间，所以注释掉了，JSON解析完后再处理\n",
        "    # detail_dict = get_post_detail_from_info_dict(info_dict)\n",
        "    # if detail_dict is None:\n",
        "    #     detail_dict = get_post_detail_from_info_dict(info_dict)\n",
        "    #     if detail_dict is None:\n",
        "    #         return None\n",
        "\n",
        "    try:\n",
        "        return {\n",
        "            # 博文ID\n",
        "            'post_id':            info_dict['postData']['postView']['id'],\n",
        "            # 博文类型  1：文本 2：图片\n",
        "            'type':               info_dict['postData']['postView']['type'],\n",
        "            # 用户ID\n",
        "            'blog_id':            info_dict['postData']['postView']['blogId'],\n",
        "            # 用户名 用于访问ta的主页\n",
        "            'blog_user_name':     info_dict['blogInfo']['blogName'],\n",
        "            # 用户的昵称 （显示的名字）\n",
        "            'blog_user_nickname': info_dict['blogInfo']['blogNickName'],\n",
        "            # 博文发出时间\n",
        "            'publish_datetime':   datetime.fromtimestamp(info_dict['postData']['postView']['publishTime'] / 1000).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            # 博文发出时间戳 ms 毫秒\n",
        "            'publish_timestamp':  info_dict['postData']['postView']['publishTime'],\n",
        "            # 所有标签 （是List）\n",
        "            'tag_list':           info_dict['postData']['postView']['tagList'],\n",
        "            # 回复数\n",
        "            'response_count':     info_dict['postData']['postCount']['responseCount'],\n",
        "            # 喜欢数\n",
        "            'favorite_count':     info_dict['postData']['postCount']['favoriteCount'],\n",
        "            # 分享数\n",
        "            'share_count':        info_dict['postData']['postCount']['shareCount'],\n",
        "            # 热度\n",
        "            'hot_count':          info_dict['postData']['postCount']['hotCount'],\n",
        "            # 暂时存着，将来应该有用\n",
        "            'permalink':          info_dict['postData']['postView']['permalink'],\n",
        "            # 博文标题\n",
        "            'title':              info_dict['postData']['postView']['title'],\n",
        "            # 是否有详情 用于判断是否解析过详情\n",
        "            'has_detail':         False,\n",
        "            # 博文内容 (解析JSON时为预览内容，并不是完整内容)\n",
        "            'content':            info_dict['postData']['postView']['digest'],\n",
        "            # 博文图片链接 (解析JSON时为空列表) []\n",
        "            'photo_links':        [],\n",
        "        }\n",
        "    except Exception:\n",
        "        return None\n",
        "# endregion\n",
        "\n",
        "# region 方法-存储\n",
        "def fetch_details_if_pass_filter(backup_list: list[dict], max_workers: int = 4) -> list[dict]:\n",
        "    \"\"\"\n",
        "    只对通过 custom_filter 的项目，且未获取详情的，执行 get_post_detail_from_backup_dict。\n",
        "    保留原始顺序与长度，未处理的项目保持不变。\n",
        "\n",
        "    Args:\n",
        "        backup_list (List[dict]): 博文列表。\n",
        "        max_workers (int): 最大并发线程数。\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: 返回更新后的列表，长度和顺序与原始列表一致。\n",
        "    \"\"\"\n",
        "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# 找出需要抓取详情的项（记录其原始索引）\n",
        "    detail_tasks = [\n",
        "        (idx, item) for idx, item in enumerate(backup_list)\n",
        "        if not item.get(\"has_detail\", False) and custom_filter(**item)\n",
        "    ]\n",
        "\n",
        "    print(f\"[详情抓取] 需要更新的博文数量: {len(detail_tasks)}\")\n",
        "\n",
        "    updated_list = deepcopy(backup_list)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        future_to_index = {\n",
        "            executor.submit(get_post_detail_from_backup_dict, item): idx\n",
        "            for idx, item in detail_tasks\n",
        "        }\n",
        "\n",
        "        for future in tqdm(as_completed(future_to_index), total=len(future_to_index), desc=\"获取博文详情\"):\n",
        "            idx = future_to_index[future]\n",
        "            try:\n",
        "                detail = future.result()\n",
        "                if detail:\n",
        "                    updated_list[idx].update(detail)\n",
        "                    updated_list[idx][\"has_detail\"] = True\n",
        "            except Exception as e:\n",
        "                print(f\"[异常] 第 {idx} 条详情抓取失败: {e}\")\n",
        "\n",
        "    return updated_list\n",
        "\n",
        "def load_list_offset_from_json(file_path: str) -> tuple[list, int]:\n",
        "    \"\"\"\n",
        "    读取包含 data 和 offset 的 JSON 文件。\n",
        "\n",
        "    Args:\n",
        "        file_path (str): JSON 文件路径，可以是普通 JSON 或 gzip 压缩文件（.gz）\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[Any], int]: 返回一个元组，包含数据列表和偏移量。如果读取失败，返回空列表和 offset=0\n",
        "    \"\"\"\n",
        "    if not file_path or not os.path.exists(file_path):\n",
        "        return [], 0\n",
        "\n",
        "    try:\n",
        "        if file_path.endswith('.gz'):\n",
        "            with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
        "                content = json.load(f)\n",
        "        else:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = json.load(f)\n",
        "\n",
        "        data = content.get('data', [])\n",
        "        offset = content.get('offset', 0)\n",
        "        return data, offset\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"读取 JSON 文件失败: {e}\")\n",
        "        return [], 0\n",
        "\n",
        "def save_list_offset_to_json(data: list, offset: int, file_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    将对象保存为 JSON 文件。\n",
        "\n",
        "    Args:\n",
        "        data (List[Any]): 要保存的数据列表。\n",
        "        offset (int): 当前的offset\n",
        "        filename (str): 目标文件名。如果为空则跳过；如果以 .gz 结尾则使用 gzip 压缩。\n",
        "\n",
        "    Returns:\n",
        "        bool: 保存成功返回 True，失败或跳过返回 False。\n",
        "    \"\"\"\n",
        "    if not file_path:\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        if os.path.dirname(file_path):\n",
        "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "        dict_to_save = {\n",
        "            \"data\": data,\n",
        "            \"offset\": offset,\n",
        "        }\n",
        "\n",
        "        tmp_path = file_path + \".tmp\"\n",
        "\n",
        "        if file_path.endswith('.gz'):\n",
        "            with gzip.open(tmp_path, 'wt', encoding='utf-8') as f:\n",
        "                json.dump(dict_to_save, f, ensure_ascii=False, indent=2)\n",
        "        else:\n",
        "            with open(tmp_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(dict_to_save, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        os.replace(tmp_path, file_path)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"JSON 保存失败: {e}\")\n",
        "        return False\n",
        "\n",
        "def safe_name(name: str) -> str:\n",
        "    \"\"\"\n",
        "    清理字符串中的非法文件名字符，包括某些不可见 Unicode 字符。\n",
        "\n",
        "    Args:\n",
        "        name (str): 原始文件名。\n",
        "\n",
        "    Returns:\n",
        "        str: 安全的、可用于保存文件的名称。\n",
        "    \"\"\"\n",
        "    name = re.sub(r'[\\\\/:*?\"<>|\\x00-\\x1F]', '_', name)\n",
        "    name = re.sub(r'[\\u200b\\u200c\\u200d\\u2060]', '', name)\n",
        "    name = re.sub(r'\\s+', ' ', name).strip()\n",
        "    return name\n",
        "\n",
        "def download_image(url: str, output_path: str, retries: int = 3) -> bool:\n",
        "    \"\"\"\n",
        "    下载图片到指定路径（重试若失败）。若文件已存在且大小相同则跳过下载\n",
        "\n",
        "    Args:\n",
        "        url (str): 图片的 URL\n",
        "        output_path (str): 下载后保存的完整路径\n",
        "        retries (int): 最大重试次数，默认 3 次\n",
        "\n",
        "    Returns:\n",
        "        bool: 下载是否成功或文件已存在且匹配。\n",
        "    \"\"\"\n",
        "    headers = {\"User-Agent\": \"okhttp/3.10.0\"}\n",
        "    if os.path.dirname(output_path):\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "    # 如果文件已存在，尝试对比大小\n",
        "    if os.path.isfile(output_path):\n",
        "        try:\n",
        "            head_resp = requests.head(url, headers=headers, timeout=10)\n",
        "            if head_resp.status_code == 200:\n",
        "                remote_size = int(head_resp.headers.get('Content-Length', 0))\n",
        "                local_size = os.path.getsize(output_path)\n",
        "                if remote_size == local_size and remote_size != 0:\n",
        "                    # 文件大小一致，认为文件相同，跳过下载\n",
        "                    return True\n",
        "        except Exception:\n",
        "            # 如果 HEAD 请求失败，继续正常下载\n",
        "            pass\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                with open(output_path, \"wb\") as f:\n",
        "                    f.write(response.content)\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            pass\n",
        "            # 可根据需要打印日志\n",
        "            # print(f\"[{attempt+1}/{retries}] 下载错误 {url}: {e}\")\n",
        "\n",
        "    return False\n",
        "\n",
        "def write_content(backup_dict: dict, output_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    将博客内容字段（HTML）转换为 Markdown，并写入指定路径。\n",
        "\n",
        "    Args:\n",
        "        backup_dict (dict): 包含博客数据的字典，需包含 'content' 字段。\n",
        "        path (str): 要写入的目标文件路径。\n",
        "\n",
        "    Returns:\n",
        "        bool: 写入成功返回 True，失败返回 False。\n",
        "    \"\"\"\n",
        "    try:\n",
        "        content = custom_content_process(**backup_dict)\n",
        "\n",
        "        # 如果内容为空或不符合自定义过滤条件，则跳过写入\n",
        "        if not custom_filter_for_text(content):\n",
        "            return False\n",
        "\n",
        "        if os.path.dirname(output_path):\n",
        "            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(content)\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"写入内容失败: {output_path}\")\n",
        "        return False\n",
        "# endregion\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# endregion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qW8ULA2BSN_",
        "outputId": "853933d5-e28d-41a2-ddb0-04620cef032f"
      },
      "outputs": [],
      "source": [
        "!ls -lh {backup_list_json_path} # 所有博文信息就在这个json 里面哟"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "ujicHwIJo_fe",
        "outputId": "e86d3769-e776-4824-daf2-00913d6dea4c"
      },
      "outputs": [],
      "source": [
        "# @markdown # 画热力图\n",
        "\n",
        "data = json.loads(\n",
        "    gzip.open(backup_list_json_path, 'rt', encoding='utf-8').read()\n",
        ")['data']\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "mpl.font_manager.fontManager.addfont('/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc')\n",
        "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP']\n",
        "\n",
        "tag_heat = defaultdict(int)\n",
        "\n",
        "for post_info in data:\n",
        "    for tag in post_info['tag_list']:\n",
        "        tag_heat[tag] += post_info['hot_count']\n",
        "\n",
        "# 1. 计算每个标签的总热度\n",
        "tag_heat = defaultdict(int)\n",
        "for post_info in data:\n",
        "    for tag in post_info['tag_list']:\n",
        "        tag_heat[tag] += post_info['hot_count']\n",
        "\n",
        "\n",
        "# 2. 选择热度最高的前n个标签\n",
        "n = 20  # 你可以修改这个值\n",
        "top_tags = sorted(tag_heat.items(), key=lambda x: -x[1])[:n]\n",
        "top_tags = [tag for tag, heat in top_tags]  # 提取标签名\n",
        "\n",
        "# 3. 统计这些标签的共现次数\n",
        "co_occurrence = defaultdict(int)\n",
        "for post_info in data:\n",
        "    # 只考虑前n个标签的共现\n",
        "    filtered_tags = [tag for tag in post_info['tag_list'] if tag in top_tags]\n",
        "    for pair in combinations(sorted(filtered_tags), 2):\n",
        "        co_occurrence[pair] += 1\n",
        "\n",
        "# 4. 构建共现矩阵\n",
        "co_matrix = np.zeros((n, n))\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        if i != j:\n",
        "            pair = tuple(sorted([top_tags[i], top_tags[j]]))\n",
        "            co_matrix[i][j] = co_occurrence.get(pair, 0)\n",
        "\n",
        "# 5. 转换为DataFrame并绘制热力图\n",
        "df_co = pd.DataFrame(co_matrix, index=top_tags, columns=top_tags)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    df_co,\n",
        "    cmap='YlOrRd',\n",
        "    annot=True,\n",
        "    fmt='g',\n",
        "    linewidths=0.5,\n",
        "    linecolor='lightgray',\n",
        "    # norm=LogNorm(),  # 使用对数颜色\n",
        "    cbar_kws={'label': '共现次数'}\n",
        ")\n",
        "plt.title(f'{tag_name} {post_year_month} {get_post_types_name(post_types)} Top {n} 热门标签共现热力图', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JDcus4SSDQdd",
        "outputId": "970e52f2-79b6-4730-a7a1-acf60e62c2a6"
      },
      "outputs": [],
      "source": [
        "data = json.loads(\n",
        "    gzip.open(backup_list_json_path, 'rt', encoding='utf-8').read()\n",
        ")['data']\n",
        "\n",
        "# 就是个dict 存放了所有的数据 注意看我的注释\n",
        "\", \".join(sorted(list(data[0].keys())))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
